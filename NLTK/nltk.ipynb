{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datax=brown.sents(categories=\"religion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['As', 'a', 'result', ',', 'although', 'we', 'still', 'make', 'use', 'of', 'this', 'distinction', ',', 'there', 'is', 'much', 'confusion', 'as', 'to', 'the', 'meaning', 'of', 'the', 'basic', 'terms', 'employed', '.'], ['Just', 'what', 'is', 'meant', 'by', '``', 'spirit', \"''\", 'and', 'by', '``', 'matter', \"''\", '?', '?'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(datax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-18aac074d8a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdocument\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "document=\"\"\n",
    "for i in range(5):\n",
    "    document+=' '.join(data[i])\n",
    "    \n",
    "print(document.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What are you doing? I guess you are leanring ML, which is a quite interesting field! Is it!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are you doing?', 'I guess you are leanring ML, which is a quite interesting field!', 'Is it!']\n"
     ]
    }
   ],
   "source": [
    "sent=sent_tokenize(text)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'are', 'you', 'doing', '?']\n"
     ]
    }
   ],
   "source": [
    "word_list=word_tokenize(sent[0])\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', '?']\n"
     ]
    }
   ],
   "source": [
    "###use ful words\n",
    "useful=[w for w in word_list if w not in sw]\n",
    "print(useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RegexpTokenizer(\"[a-zA-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=tokenizer.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not', 'about', 'them', 'do', 'too', 's', 'am', \"wouldn't\", 'same', 'such', 'aren', 'hasn', 'having', 'because', 'myself', 'where', 'be', 'did', 'or', 'weren', 'to', 'himself', 'just', \"it's\", 'here', 'this', 'those', \"you're\", 'very', 'being', \"won't\", 'that', \"you'll\", 'yourself', 'didn', 'there', 'each', 'are', 'wouldn', \"isn't\", 'does', 'at', 'other', 'any', 'were', 'out', 'when', 'what', 'your', 'down', 'been', 'so', 't', 'herself', 'was', 'have', \"wasn't\", 'of', 'yourselves', 'hers', 'in', 'few', 'should', 'now', 'doesn', 'which', 'over', \"doesn't\", 'm', \"hadn't\", 'ours', \"mightn't\", 'doing', 'on', 'she', 'they', 'while', 'whom', \"you've\", 'our', 'yours', 'has', 'and', 'further', 'before', 'most', \"mustn't\", \"shouldn't\", \"should've\", \"hasn't\", 'off', 'between', 'her', 'the', 'hadn', 'no', 'me', \"she's\", 'don', 'mustn', 'why', 'until', 'under', 'as', 'is', \"don't\", 'once', \"needn't\", 'it', 'o', 'will', 'both', 'you', 'its', 'after', 'all', 'into', \"shan't\", \"weren't\", 'these', 'my', 'how', 'haven', 'he', 'wasn', 'for', 'isn', 've', 'won', 'y', 'if', 'a', 'but', 'than', 'll', 'their', 'shouldn', 'ma', 'itself', 'during', \"didn't\", \"aren't\", 'then', 'couldn', 're', 'own', 'only', 'we', \"that'll\", \"couldn't\", 'more', 'through', 'ain', 'ourselves', 'theirs', 'by', 'him', 'needn', 'against', 'again', 'from', 'd', 'themselves', 'below', 'can', 'above', 'shan', 'nor', 'an', 'his', \"haven't\", 'with', 'i', \"you'd\", 'who', 'up', 'mightn', 'had', 'some'}\n"
     ]
    }
   ],
   "source": [
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words=[w for w in word_list if w not in ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guess', 'leanring', 'ml', 'quite', 'interesting', 'field']\n"
     ]
    }
   ],
   "source": [
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer,PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quickli'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=LancasterStemmer()\n",
    "ps=PorterStemmer()\n",
    "ps.stem(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wor=[]\n",
    "for w in useful_words:\n",
    "    s=ss.stem(w)\n",
    "    wor.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guess', 'leanr', 'ml', 'quit', 'interest', 'field']\n"
     ]
    }
   ],
   "source": [
    "print(wor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        'Indian cricket team will win 2019 World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 13, 'cricket': 7, 'team': 32, 'will': 38, 'win': 39, '2019': 0, 'world': 41, 'cup': 8, 'says': 28, 'capt': 5, 'virat': 36, 'kohli': 15, 'be': 4, 'held': 12, 'at': 2, 'sri': 30, 'lanka': 16, 'we': 37, 'next': 20, 'lok': 18, 'sabha': 27, 'elections': 9, 'confident': 6, 'pm': 24, 'the': 33, 'nobel': 21, 'laurate': 17, 'won': 40, 'hearts': 11, 'of': 22, 'people': 23, 'movie': 19, 'raazi': 25, 'is': 14, 'an': 1, 'exciting': 10, 'spy': 29, 'thriller': 34, 'based': 3, 'upon': 35, 'real': 26, 'story': 31}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      "  1 0 2 1 0 2]\n",
      " [0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 3 0 0\n",
      "  0 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1\n",
      "  0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytok=RegexpTokenizer('[a-zA-Z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(tokenizer=mytok.tokenize,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 2 1 1 1 1 0 0 0 0 0 0 0\n",
      "  0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 1 0 0 0 2 1 1 1 0 1 1 0 0 1 1 0 0 0 2 2 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1 1 0 0 1 1 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      "  0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(cv.fit_transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 41,\n",
       " 'cricket': 21,\n",
       " 'team': 93,\n",
       " 'will': 116,\n",
       " 'win': 122,\n",
       " 'world': 130,\n",
       " 'cup': 24,\n",
       " 'says': 82,\n",
       " 'capt': 15,\n",
       " 'virat': 110,\n",
       " 'kohli': 50,\n",
       " 'be': 12,\n",
       " 'held': 38,\n",
       " 'at': 6,\n",
       " 'sri': 90,\n",
       " 'lanka': 53,\n",
       " 'indian cricket': 42,\n",
       " 'cricket team': 22,\n",
       " 'team will': 94,\n",
       " 'will win': 119,\n",
       " 'win world': 125,\n",
       " 'world cup': 131,\n",
       " 'cup says': 25,\n",
       " 'says capt': 83,\n",
       " 'capt virat': 16,\n",
       " 'virat kohli': 111,\n",
       " 'kohli world': 51,\n",
       " 'cup will': 27,\n",
       " 'will be': 117,\n",
       " 'be held': 13,\n",
       " 'held at': 39,\n",
       " 'at sri': 7,\n",
       " 'sri lanka': 91,\n",
       " 'indian cricket team': 43,\n",
       " 'cricket team will': 23,\n",
       " 'team will win': 95,\n",
       " 'will win world': 121,\n",
       " 'win world cup': 126,\n",
       " 'world cup says': 132,\n",
       " 'cup says capt': 26,\n",
       " 'says capt virat': 84,\n",
       " 'capt virat kohli': 17,\n",
       " 'virat kohli world': 112,\n",
       " 'kohli world cup': 52,\n",
       " 'world cup will': 133,\n",
       " 'cup will be': 28,\n",
       " 'will be held': 118,\n",
       " 'be held at': 14,\n",
       " 'held at sri': 40,\n",
       " 'at sri lanka': 8,\n",
       " 'we': 113,\n",
       " 'next': 63,\n",
       " 'lok': 57,\n",
       " 'sabha': 79,\n",
       " 'elections': 29,\n",
       " 'confident': 18,\n",
       " 'pm': 73,\n",
       " 'we will': 114,\n",
       " 'win next': 123,\n",
       " 'next lok': 64,\n",
       " 'lok sabha': 58,\n",
       " 'sabha elections': 80,\n",
       " 'elections says': 30,\n",
       " 'says confident': 85,\n",
       " 'confident indian': 19,\n",
       " 'indian pm': 44,\n",
       " 'we will win': 115,\n",
       " 'will win next': 120,\n",
       " 'win next lok': 124,\n",
       " 'next lok sabha': 65,\n",
       " 'lok sabha elections': 59,\n",
       " 'sabha elections says': 81,\n",
       " 'elections says confident': 31,\n",
       " 'says confident indian': 86,\n",
       " 'confident indian pm': 20,\n",
       " 'the': 96,\n",
       " 'nobel': 66,\n",
       " 'laurate': 54,\n",
       " 'won': 127,\n",
       " 'hearts': 35,\n",
       " 'of': 69,\n",
       " 'people': 72,\n",
       " 'the nobel': 101,\n",
       " 'nobel laurate': 67,\n",
       " 'laurate won': 55,\n",
       " 'won the': 128,\n",
       " 'the hearts': 97,\n",
       " 'hearts of': 36,\n",
       " 'of the': 70,\n",
       " 'the people': 103,\n",
       " 'the nobel laurate': 102,\n",
       " 'nobel laurate won': 68,\n",
       " 'laurate won the': 56,\n",
       " 'won the hearts': 129,\n",
       " 'the hearts of': 98,\n",
       " 'hearts of the': 37,\n",
       " 'of the people': 71,\n",
       " 'movie': 60,\n",
       " 'raazi': 74,\n",
       " 'is': 47,\n",
       " 'an': 3,\n",
       " 'exciting': 32,\n",
       " 'spy': 87,\n",
       " 'thriller': 104,\n",
       " 'based': 9,\n",
       " 'upon': 107,\n",
       " 'a': 0,\n",
       " 'real': 77,\n",
       " 'story': 92,\n",
       " 'the movie': 99,\n",
       " 'movie raazi': 61,\n",
       " 'raazi is': 75,\n",
       " 'is an': 48,\n",
       " 'an exciting': 4,\n",
       " 'exciting indian': 33,\n",
       " 'indian spy': 45,\n",
       " 'spy thriller': 88,\n",
       " 'thriller based': 105,\n",
       " 'based upon': 10,\n",
       " 'upon a': 108,\n",
       " 'a real': 1,\n",
       " 'real story': 78,\n",
       " 'the movie raazi': 100,\n",
       " 'movie raazi is': 62,\n",
       " 'raazi is an': 76,\n",
       " 'is an exciting': 49,\n",
       " 'an exciting indian': 5,\n",
       " 'exciting indian spy': 34,\n",
       " 'indian spy thriller': 46,\n",
       " 'spy thriller based': 89,\n",
       " 'thriller based upon': 106,\n",
       " 'based upon a': 11,\n",
       " 'upon a real': 109,\n",
       " 'a real story': 2}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20789632 0.         0.20789632 0.         0.20789632 0.20789632\n",
      "  0.         0.20789632 0.41579263 0.         0.         0.\n",
      "  0.20789632 0.13269753 0.         0.20789632 0.20789632 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16390784 0.\n",
      "  0.20789632 0.         0.20789632 0.         0.         0.\n",
      "  0.20789632 0.         0.32781569 0.16390784 0.         0.41579263]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.32840433 0.         0.         0.32840433 0.         0.\n",
      "  0.         0.20961623 0.         0.         0.         0.\n",
      "  0.32840433 0.         0.32840433 0.         0.         0.\n",
      "  0.32840433 0.         0.         0.32840433 0.25891775 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.32840433 0.25891775 0.25891775 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.29368184\n",
      "  0.         0.         0.         0.         0.         0.29368184\n",
      "  0.         0.         0.         0.29368184 0.29368184 0.29368184\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.69462641 0.         0.\n",
      "  0.         0.         0.         0.         0.29368184 0.        ]\n",
      " [0.         0.28832691 0.         0.28832691 0.         0.\n",
      "  0.         0.         0.         0.         0.28832691 0.\n",
      "  0.         0.18403533 0.28832691 0.         0.         0.\n",
      "  0.         0.28832691 0.         0.         0.         0.\n",
      "  0.         0.28832691 0.28832691 0.         0.         0.28832691\n",
      "  0.         0.28832691 0.         0.22732025 0.28832691 0.28832691\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "vd = tf.fit_transform(corpus).toarray()\n",
    "print(vd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup=fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-fec5cccaee12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewsgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x=newsgroup.data\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "y=newsgroup.target_names\n",
    "\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=newsgroup.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ab@nova.cc.purdue.edu (Allen B)\n",
      "Subject: Re: TIFF: philosophical significance of 42\n",
      "Organization: Purdue University\n",
      "Lines: 39\n",
      "\n",
      "In article <prestonm.735400848@cs.man.ac.uk> prestonm@cs.man.ac.uk (Martin  \n",
      "Preston) writes:\n",
      "> Why not use the PD C library for reading/writing TIFF files? It took me a\n",
      "> good 20 minutes to start using them in your own app.\n",
      "\n",
      "I certainly do use it whenever I have to do TIFF, and it usually works\n",
      "very well.  That's not my point.  I'm >philosophically< opposed to it\n",
      "because of its complexity.\n",
      "\n",
      "This complexity has led to some programs' poor TIFF writers making\n",
      "some very bizarre files, other programs' inability to load TIFF\n",
      "images (though they'll save them, of course), and a general\n",
      "inability to interchange images between different environments\n",
      "despite the fact they all think they understand TIFF.\n",
      "\n",
      "As the saying goes, \"It's not me I'm worried about- it's all the\n",
      ">other<  assholes out there!\"  I've had big trouble with misuse and\n",
      "abuse of TIFF over the years, and I chalk it all up to the immense (and\n",
      "unnecessary) complexity of the format.\n",
      "\n",
      "In the words of the TIFF 5.0 spec, Appendix G, page G-1 (capitalized\n",
      "emphasis mine):\n",
      "\n",
      "\"The only problem with this sort of success is that TIFF was designed\n",
      "to be powerful and flexible, at the expense of simplicity.  It takes a\n",
      "fair amount of effort to handle all the options currently defined in\n",
      "this specification (PROBABLY NO APPLICATION DOES A COMPLETE JOB),\n",
      "and that is currently the only way you can be >sure< that you will be\n",
      "able to import any TIFF image, since there are so many\n",
      "image-generating applications out there now.\"\n",
      "\n",
      "\n",
      "If a program (or worse all applications) can't read >every< TIFF\n",
      "image, that means there are some it won't- some that I might have to\n",
      "deal with.  Why would I want my images to be trapped in that format?  I\n",
      "don't and neither should anyone who agrees with my reasoning- not\n",
      "that anyone does, of course! :-)\n",
      "\n",
      "ab\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=x[13]\n",
    "data1=x[16]\n",
    "data2=x[17]\n",
    "print(data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=sent_tokenize(data)[2:]\n",
    "sv=sent_tokenize(data1)[2:]\n",
    "sw=sent_tokenize(data2)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "for i in range(3):\n",
    "    pass\n",
    "data_train1,data_test1=train_test_split(ss)\n",
    "data_train2,data_test2=train_test_split(sv)\n",
    "data_train3,data_test3=train_test_split(sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Option', 'C', '-', 'Single', 'Core', 'Launch', 'Station', '.'], ['-', 'Permanent', 'Human', 'Presence', 'Capability', 'occurs', 'after', '10', 'flights', ',', 'by', 'keeping', 'one', 'Orbiter', 'on-orbit', 'to', 'use', 'as', 'an', 'ACRV', '(', 'so', 'sometimes', 'there', 'would', 'be', 'two', 'Orbiters', 'on-orbit', '-', 'the', 'ACRV', ',', 'and', 'the', 'second', 'one', 'that', 'comes', 'up', 'for', 'Logistics', '&', 'Re-supply', ')', '.'], ['-', '7', 'floors', ',', 'center', 'passageway', 'between', 'floors', '-', '10', 'kW', 'of', 'housekeeping', 'power', '-', 'graceful', 'degradation', 'with', 'failures', '(', '8', 'power', 'channels', ',', '4', 'thermal', 'loops', ',', 'dual', 'environmental', 'control', '&', 'life', 'support', 'system', ')', '-', 'increased', 'crew', 'time', 'for', 'utilization', '-', '1', 'micro-g', 'thru', 'out', 'the', 'core', 'module'], ['The', 'U.S', '.'], ['The', 'vehicle', 'flies', 'in', 'an', '``', 'arrow', 'mode', \"''\", 'to', 'optimize', 'the', 'microgravity', 'environment', '.'], ['Add', 'a', '3rd', 'power', 'module', ',', 'the', 'U.S.', 'habitation', 'module', ',', 'and', 'an', 'ACRV', '(', 'Assured', 'Crew', 'Return', 'Vehicle', ')', '.'], ['Add', 'the', 'NASDA', '&', 'ESA', 'modules', ',', 'and', 'add', 'another', '20', 'kW', 'of', 'power', '-', 'Permanent', 'Human', 'Presence', 'Capability', '.'], ['Yo', 'Ken', ',', 'let', \"'s\", 'keep', 'on-top', 'of', 'things', '!'], ['Most', 'of', 'the', 'systems', 'currently', 'on', 'SSF', 'are', 'used', 'as-is', 'in', 'this', 'option', ',', 'with', 'the', 'exception', 'of', 'the', 'data', 'management', 'system', ',', 'which', 'has', 'major', 'changes', '.'], ['This', 'option', 'looks', 'alot', 'like', 'the', 'existing', 'SSF', 'design', ',', 'which', 'we', 'have', 'all', 'come', 'to', 'know', 'and', 'love', ':', ')', 'This', 'option', 'assumes', 'a', 'lightweight', 'external', 'tank', 'is', 'available', 'for', 'use', 'on', 'all', 'SSF', 'assembly', 'flights', '(', 'so', 'does', 'option', '``', 'A', \"''\", ')', '.'], ['-', 'A', '``', 'Power', 'Station', 'Capability', \"''\", 'is', 'obtained', 'in', '3', 'Shuttle', 'Flights', '.'], ['The', 'build-up', 'occurs', 'in', 'six', 'phases', ':', '-', 'Initial', 'Research', 'Capability', 'reached', 'after', '3', 'flights', '.'], ['Power', 'is', 'transferred', 'from', 'the', 'vehicle', 'to', 'the', 'Orbiter/Spacelab', ',', 'when', 'it', 'visits', '.'], ['Currently', ',', 'there', 'are', 'three', 'options', 'being', 'considered', ',', 'as', 'presented', 'to', 'the', 'advisory', 'panel', 'meeting', 'yesterday', '(', 'and', 'as', 'reported', 'in', 'today', \"'s\", 'Times', ')', '.'], ['-', 'After', '20', 'flights', ',', 'the', 'Internationals', 'are', 'on-board', '.'], ['More', 'power', ',', 'the', 'Habitation', 'module', ',', 'and', 'an', 'ACRV', 'are', 'added', 'to', 'finish', 'the', 'assembly', 'in', '24', 'flights', '.'], ['Today', \"'s\", '(', '4/23', ')', 'edition', 'of', 'the', 'New', 'York', 'Times', 'reports', 'that', \"O'Connor\", 'told', 'the', 'panel', 'that', 'some', 'redesign', 'proposals', 'have', 'been', 'dropped', ',', 'such', 'as', 'using', 'the', '``', 'giant', 'external', 'fuel', 'tanks', 'used', 'in', 'launching', 'space', 'shuttles', ',', \"''\", 'and', 'building', 'a', '``', 'station', 'around', 'an', 'existing', 'space', 'shuttle', 'with', 'its', 'wings', 'and', 'tail', 'removed', '.', \"''\"], ['-', 'International', 'Human', 'Tended', '.'], ['The', 'bus', 'provides', 'propulsion', ',', 'GN', '&', 'C', 'Communications', ',', '&', 'Data', 'Management', '.'], ['-', 'Man-Tended', 'Capability', '(', 'Griffin', 'has', 'not', 'yet', 'adopted', 'non-sexist', 'language', ')', 'is', 'achieved', 'after', '8', 'flights', '.'], ['Option', '``', 'A', \"''\", '-', 'Low', 'Cost', 'Modular', 'Approach', 'This', 'option', 'is', 'being', 'studied', 'by', 'a', 'team', 'from', 'MSFC', '.'], ['The', 'Shuttle', 'can', 'be', 'docked', 'to', 'the', 'station', 'for', '60', 'day', 'missions', '.'], ['Lab', 'is', 'deployed', ',', 'and', '1', 'solar', 'power', 'module', 'provides', '20', 'kW', 'of', 'power', '.'], ['Both', 'the', '``', 'External', 'Tank', \"''\", 'and', \"''\", 'Wingless', 'Orbiter', \"''\", 'options', 'have', 'been', 'deleted', 'from', 'the', 'SSF', 'redesign', 'options', 'list', '.'], ['Option', '``', 'B', \"''\", '-', 'Space', 'Station', 'Freedom', 'Derived', 'The', 'Option', '``', 'B', \"''\", 'team', 'is', 'based', 'at', 'LaRC', ',', 'and', 'is', 'lead', 'by', 'Mike', 'Griffin', '.'], ['-', 'A', '``', 'Two', 'Fault', 'Tolerance', 'Capability', \"''\", 'is', 'achieved', 'after', '14', 'flights', ',', 'with', 'the', 'addition', 'of', 'a', '2nd', 'power', 'module', ',', 'another', 'thermal', 'control', 'system', 'radiator', ',', 'and', 'more', 'propulsion', 'modules', '.'], ['This', 'is', 'the', 'JSC', 'lead', 'option', '.'], ['Basically', ',', 'you', 'take', 'a', '23', 'ft', 'diameter', 'cylinder', 'that', \"'s\", '92', 'ft', 'long', ',', 'slap', '3', 'Space', 'Shuttle', 'Main', 'Engines', 'on', 'the', 'backside', ',', 'put', 'a', 'nose', 'cone', 'on', 'the', 'top', ',', 'attached', 'it', 'to', 'a', 'regular', 'shuttle', 'external', 'tank', 'and', 'a', 'regular', 'set', 'of', 'solid', 'rocket', 'motors', ',', 'and', 'launch', 'the', 'can', '.'], ['Lockheed', 'developed', 'this', 'for', 'the', 'Air', 'Force', '.']]\n"
     ]
    }
   ],
   "source": [
    "def tok(data):\n",
    "    ss=[]\n",
    "    for i in range(len(data)):\n",
    "        a=word_tokenize(data[i])\n",
    "        ss.append(a)\n",
    "    return ss\n",
    "s1tr=tok(data_train1)\n",
    "print(s1tr)\n",
    "s1te=tok(data_test1)\n",
    "s2tr=tok(data_train2)\n",
    "s2te=tok(data_test2)\n",
    "s3tr=tok(data_train3)\n",
    "s3te=tok(data_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-ede03577f337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0museful_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0museful_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0ms1tr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0ms2tr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0ms3tr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms3tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-ede03577f337>\u001b[0m in \u001b[0;36msto\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0museful_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0museful_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0ms1tr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-ede03577f337>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0museful_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0museful_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0ms1tr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "ss=set(stopwords.words('english'))\n",
    "def sto(data):\n",
    "    useful_words=[w for w in data if w not in ss]\n",
    "    return useful_words\n",
    "s1tr=sto(s1tr)\n",
    "s2tr=sto(s2tr)\n",
    "s3tr=sto(s3tr)\n",
    "s1te=sto(s1te)\n",
    "s12te=sto(s2te)\n",
    "s3te=sto(s3te)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words=[w for w in data if w not in ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=''\n",
    "for i in data:\n",
    "    document+=' '.join(i)\n",
    "    \n",
    "print(document.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
